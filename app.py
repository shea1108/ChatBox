from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import gradio as gr

# Load tokenizer & model TinyLlama
tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
model = AutoModelForCausalLM.from_pretrained(
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    torch_dtype=torch.float32,   # D√πng torch.float32 n·∫øu g·∫∑p v·∫•n ƒë·ªÅ v·ªõi float16
    device_map="auto"
)

# Kh·ªüi t·∫°o l·ªãch s·ª≠ h·ªôi tho·∫°i, gi·ªõi h·∫°n t·ªëi ƒëa s·ªë c√¢u h·ªèi
def init_history(max_history=5):
    return [{
        "role": "system",
        "content": (
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o th√¥ng minh t√™n l√† Th·∫Øng. "
            "B·∫°n n√≥i ti·∫øng Vi·ªát m·ªôt c√°ch l·ªãch s·ª±, th√¢n thi·ªán v√† h·ªØu √≠ch. "
            "Khi ng∆∞·ªùi d√πng n√≥i 'Xin ch√†o' ho·∫∑c h·ªèi 'B·∫°n l√† ai?', b·∫°n n√™n tr·∫£ l·ªùi: 'T√¥i l√† Th·∫Øng, tr·ª£ l√Ω ·∫£o c·ªßa b·∫°n.' "
            "Lu√¥n lu√¥n c·ªë g·∫Øng h·ªó tr·ª£ ng∆∞·ªùi d√πng t·ªët nh·∫•t."
        )
    }]

# H√†m gi·ªõi h·∫°n s·ªë l∆∞·ª£ng l·ªãch s·ª≠ tr√≤ chuy·ªán
def limit_history(history, max_length=5):
    return history[-max_length:]

# H√†m t·∫°o ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh
def generate_reply(user_input, history):
    if not history or not isinstance(history, list):
        history = init_history()

    # Gi·ªõi h·∫°n l·ªãch s·ª≠ ƒë·ªÉ gi·∫£m s·ªë token ƒë·∫ßu v√†o
    history = limit_history(history, max_length=5)

    # Th√™m c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠
    history.append({"role": "user", "content": user_input})

    # T·∫°o prompt d·∫°ng chat
    prompt = "".join([f"{entry['content']}\n" for entry in history])

    # Chuy·ªÉn ƒë·ªïi prompt th√†nh input_ids
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

    # Sinh ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh
    output = model.generate(
        input_ids,
        max_new_tokens=150,  # Gi·ªõi h·∫°n s·ªë token ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )

    # Tr√≠ch xu·∫•t ph·∫£n h·ªìi v√† lo·∫°i b·ªè ph·∫ßn "assistant:"
    response = tokenizer.decode(output[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()

    # Lo·∫°i b·ªè ph·∫ßn "assistant:" n·∫øu c√≥
    response = response.replace("assistant:", "").strip()

    history.append({"role": "assistant", "content": response})
    
    return history, history

# Giao di·ªán Gradio
with gr.Blocks() as demo:
    gr.Markdown("## ü§ñ Tr·ª£ l√Ω ·∫£o Th·∫Øng ‚Äì Phi√™n b·∫£n TinyLlama nhanh nh·∫π!")

    chatbot = gr.Chatbot(type="messages", label="Th·∫Øng AI")
    state = gr.State(init_history())  # Kh·ªüi t·∫°o l·ªãch s·ª≠ h·ªôi tho·∫°i

    with gr.Row():
        msg = gr.Textbox(placeholder="Nh·∫≠p c√¢u h·ªèi t·∫°i ƒë√¢y...", show_label=False)
        send_btn = gr.Button("G·ª≠i")

    send_btn.click(generate_reply, inputs=[msg, state], outputs=[chatbot, state])
    msg.submit(generate_reply, inputs=[msg, state], outputs=[chatbot, state])

if __name__ == "__main__":
    demo.launch()
